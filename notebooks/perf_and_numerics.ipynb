{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prelude",
   "id": "1a1ed627a0d69b55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T21:44:28.200283Z",
     "start_time": "2025-10-23T21:44:27.839322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"0\"\n",
    "\n",
    "def prelude():\n",
    "    def _clear_vars():\n",
    "        for _n in list(globals()):\n",
    "            if _n != \"prelude\" and _n.startswith(\"_\") and _n not in (\"In\",\"Out\",\"get_ipython\",\"exit\",\"quit\"):\n",
    "                del globals()[_n]\n",
    "\n",
    "    _clear_vars()\n",
    "\n",
    "    G = globals()\n",
    "\n",
    "    import contextlib, torch\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def dev_sync(device: str):\n",
    "        is_cuda = torch.cuda.is_available() and str(device).startswith(\"cuda\")\n",
    "        is_mps = hasattr(torch, \"mps\") and torch.mps.is_available() and str(device).startswith(\"mps\")\n",
    "        if is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        elif is_mps:\n",
    "            torch.mps.synchronize()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if is_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "            elif is_mps:\n",
    "                torch.mps.synchronize()\n",
    "    G[\"dev_sync\"] = dev_sync\n",
    "\n",
    "# pretend export to help static analysis\n",
    "def dev_sync(device: str):\n",
    "    pass\n",
    "\n",
    "prelude()"
   ],
   "id": "de0d4335326716d2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device Copy Overhead",
   "id": "6fad247121126c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T21:44:30.635212Z",
     "start_time": "2025-10-23T21:44:30.633065Z"
    }
   },
   "cell_type": "code",
   "source": "prelude()",
   "id": "93f3ca7b0a98e429",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:57:41.844379Z",
     "start_time": "2025-10-23T22:57:41.842400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"0\"\n",
    "os.environ[\"PYTORCH_MPS_FAST_MATH\"] = \"1\""
   ],
   "id": "ac793a3755e8997c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T00:50:56.291473Z",
     "start_time": "2025-10-24T00:50:30.548713Z"
    }
   },
   "source": [
    "import time, torch, torch.nn.functional as F, pandas as pd\n",
    "V, iters = 50000, 2048\n",
    "dtypes = [torch.bfloat16, torch.float16, torch.float32]\n",
    "device = \"mps\"\n",
    "target_devices = [\"cpu\", \"mps\"]\n",
    "\n",
    "print(f\"tensor shape: ({V},), iters: {iters}, dtypes: {dtypes}, target_devices: {target_devices}\")\n",
    "rows = []\n",
    "for device in target_devices:\n",
    "    for dtype in dtypes:\n",
    "        X = torch.randn(V, device=device, dtype=dtype)\n",
    "        p = F.softmax(X, -1)\n",
    "\n",
    "        # same-device .to(...)\n",
    "        for _ in range(10): \n",
    "            _ = X.to(device)\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = X.to(device)\n",
    "        t_same = time.perf_counter() - s\n",
    "        t_same = 1e3 * t_same / iters\n",
    "\n",
    "        # cross-device .to(...)\n",
    "        target_device = \"cpu\" if device == \"mps\" else \"mps\"\n",
    "        for _ in range(10): \n",
    "            _ = X.to(target_device)\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = X.to(target_device)\n",
    "        t_cross = time.perf_counter() - s\n",
    "        t_cross = 1e3 * t_cross / iters\n",
    "\n",
    "        # argmax -> CPU scalar\n",
    "        for _ in range(10):\n",
    "            _ = int(torch.argmax(X))\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters):\n",
    "                _ = torch.argmax(X)\n",
    "        t_argmax = time.perf_counter() - s\n",
    "        t_argmax = 1e3 * t_argmax / iters\n",
    "\n",
    "        # argmax -> CPU scalar\n",
    "        for _ in range(10): \n",
    "            _ = int(torch.argmax(X).item())\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = torch.argmax(X).item()\n",
    "        t_argmax_item = time.perf_counter() - s\n",
    "        t_argmax_item = 1e3 * t_argmax_item / iters\n",
    "\n",
    "        # multinomial (tensor result)\n",
    "        for _ in range(10): \n",
    "            _ = torch.multinomial(p, 1).item()\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = torch.multinomial(p, 1)\n",
    "        t_mult = time.perf_counter() - s\n",
    "        t_mult = 1e3 * t_mult / iters\n",
    "\n",
    "        # multinomial -> CPU scalar\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = torch.multinomial(p, 1).item()\n",
    "        t_mult_item = time.perf_counter() - s\n",
    "        t_mult_item = 1e3 * t_mult_item / iters\n",
    "\n",
    "        # tensor -> CPU multinomial -> CPU scalar\n",
    "        for _ in range(10): \n",
    "            _ = torch.multinomial(p.to(\"cpu\"), 1).item()\n",
    "        with dev_sync(device):\n",
    "            s = time.perf_counter()\n",
    "            for _ in range(iters): \n",
    "                _ = torch.multinomial(p.to(\"cpu\"), 1).item()  # simulating CPU sampler\n",
    "        t_cpu_mult_item = time.perf_counter() - s\n",
    "        t_cpu_mult_item = 1e3 * t_cpu_mult_item / iters\n",
    "\n",
    "        rows.append({\n",
    "            \"device\": device,\n",
    "            \"dtype\": str(dtype).replace(\"torch.\", \"\"),\n",
    "            \"to_same_dev\": t_same,\n",
    "            \"to_different_dev\": t_cross,\n",
    "            \"argmax\": t_argmax,\n",
    "            \"argmax_item\": t_argmax_item,\n",
    "            \"multinomial\": t_mult,\n",
    "            \"multinomial_item\": t_mult_item,\n",
    "            \"cpu_multinomial_item\": t_cpu_mult_item,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df[[\n",
    "    \"device\",\n",
    "    \"dtype\",\n",
    "    \"to_same_dev\",\n",
    "    \"to_different_dev\",\n",
    "    \"argmax\",\n",
    "    \"argmax_item\",\n",
    "    \"multinomial\",\n",
    "    \"multinomial_item\",\n",
    "    \"cpu_multinomial_item\",\n",
    "]].set_index(\"device\").sort_index()\n",
    "\n",
    "print(df.to_string(float_format=lambda x: f\"{x:.4f}\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: (50000,), iters: 2048, dtypes: [torch.bfloat16, torch.float16, torch.float32], target_devices: ['cpu', 'mps']\n",
      "           dtype  to_same_dev  to_different_dev  argmax  argmax_item  multinomial  multinomial_item  cpu_multinomial_item\n",
      "device                                                                                                                   \n",
      "cpu     bfloat16       0.0003            0.1275  0.0360       0.0361       0.5669            0.5573                0.5546\n",
      "cpu      float16       0.0002            0.1165  0.0233       0.0233       0.5250            0.5275                0.5281\n",
      "cpu      float32       0.0002            0.1150  0.0458       0.0458       0.5990            0.5649                0.5562\n",
      "mps     bfloat16       0.0002            0.1104  0.0217       0.1474       0.5712            0.7345                0.7183\n",
      "mps      float16       0.0002            0.1154  0.0161       0.1508       0.6070            0.7264                0.6933\n",
      "mps      float32       0.0002            0.1165  0.0136       0.1528       0.6074            0.7255                0.7290\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pseudo-torch.multinomial",
   "id": "e6a12c96ae49e080"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T01:47:46.815482Z",
     "start_time": "2025-10-24T01:47:30.169311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prelude()\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def _gumbel_multinomial(X, unused=1):\n",
    "    u = torch.rand(X.shape, device=X.device, dtype=X.dtype)\n",
    "    g = -torch.log(-torch.log(u.clamp_min_(1e-6)))\n",
    "    return torch.argmax(X + g, dim=-1)\n",
    "\n",
    "def _pseudo_multinomial(p, unused=1):\n",
    "    q = torch.empty_like(p)\n",
    "    # validity checks (as in the torch kernel)\n",
    "    _ = ((p.max() < float('inf')) & (p.min() >= 0)).item()\n",
    "    _ = (p.sum() == 0).item() if p.dim()==1 else ((p.sum(1) == 0).sum().item())\n",
    "    q.exponential_(1.0)\n",
    "    r = p / q\n",
    "    return torch.argmax(r)\n",
    "\n",
    "devices = ['mps', 'cpu']\n",
    "dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "iters = 2048\n",
    "V = 50000\n",
    "funcs = [_gumbel_multinomial, _pseudo_multinomial, torch.multinomial]\n",
    "perf_rows = []\n",
    "numerics_rows = []\n",
    "with torch.inference_mode():\n",
    "    for device in devices:\n",
    "        for dtype in dtypes:\n",
    "            X = torch.randn(V, device=device, dtype=dtype)\n",
    "            P = F.softmax(X, -1)\n",
    "            for func in funcs:\n",
    "                arg = P if func == torch.multinomial else X\n",
    "                with dev_sync(device):\n",
    "                    t0 = time.perf_counter()\n",
    "                    for i in range(iters):\n",
    "                        func(arg,1)\n",
    "                t1 = time.perf_counter()\n",
    "                rate = 1e3*(t1-t0)/iters\n",
    "                perf_rows.append({\n",
    "                    \"device\": device,\n",
    "                    \"func\": func.__name__,\n",
    "                    \"dtype\": str(dtype).replace(\"torch.\", \"\"),\n",
    "                    \"avg (ms)\": rate,\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(perf_rows)\n",
    "df = df[[\n",
    "    \"device\",\n",
    "    \"func\",\n",
    "    \"dtype\",\n",
    "    \"avg (ms)\",\n",
    "]].set_index([\"func\",\"device\"]).sort_index(level=[\"func\", \"device\"])\n",
    "\n",
    "print(df.to_string(float_format=lambda x: f\"{x:.3f}\"))\n"
   ],
   "id": "f6e7010f917febdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               dtype  avg (ms)\n",
      "func                device                    \n",
      "_gumbel_multinomial cpu      float32     0.350\n",
      "                    cpu      float16     0.319\n",
      "                    cpu     bfloat16     0.359\n",
      "                    mps      float32     0.056\n",
      "                    mps      float16     0.051\n",
      "                    mps     bfloat16     0.057\n",
      "_pseudo_multinomial cpu      float32     0.563\n",
      "                    cpu      float16     0.527\n",
      "                    cpu     bfloat16     0.561\n",
      "                    mps      float32     0.623\n",
      "                    mps      float16     0.619\n",
      "                    mps     bfloat16     0.580\n",
      "multinomial         cpu      float32     0.575\n",
      "                    cpu      float16     0.533\n",
      "                    cpu     bfloat16     0.556\n",
      "                    mps      float32     0.610\n",
      "                    mps      float16     0.606\n",
      "                    mps     bfloat16     0.577\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T03:01:27.901856Z",
     "start_time": "2025-10-24T03:01:27.832590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _topk_filter(x, k):\n",
    "    if k is None or k <= 0 or k >= x.size(-1):\n",
    "        return x\n",
    "    v, i = torch.topk(x, k)\n",
    "    y = torch.full_like(x, float(\"-inf\"))\n",
    "    y.scatter_(dim=-1, index=i, src=v)\n",
    "    return y\n",
    "\n",
    "def _topk_filter_simple(x, k):\n",
    "    \"\"\"\n",
    "    bug: if there are ties then returned tensor will have >k entries\n",
    "    \"\"\"\n",
    "    v, _ = torch.topk(x, k)\n",
    "    y = torch.where(x >= v[-1], x, torch.tensor(float(\"-inf\"), device=x.device))\n",
    "    return y\n",
    "\n",
    "# dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "dtypes = [ torch.float16,torch.float16, torch.float16,]\n",
    "k = 100\n",
    "V = 50000\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "device = \"mps\"\n",
    "for dtype in dtypes:\n",
    "    X = torch.randn(V, dtype=dtype, device=device)\n",
    "    x_tf = _topk_filter(X, k)\n",
    "    x_fs = _topk_filter_simple(X, k)\n",
    "\n",
    "    rtol, atol = 1e-3, 1e-3\n",
    "    a = x_tf.to(torch.float32)\n",
    "    b = x_fs.to(torch.float32)\n",
    "\n",
    "    fin = torch.isfinite(a) & torch.isfinite(b)\n",
    "    if fin.any():\n",
    "        diff = (a[fin] - b[fin])\n",
    "        print(\"max_abs_err(finite):\", diff.abs().max().item())\n",
    "        print(\"mean_abs_err(finite):\", diff.abs().mean().item())\n",
    "        den = a[fin].abs().max().clamp_min(1e-12)\n",
    "        print(\"rel_err(finite):\", (diff.abs().max() / den).item())\n",
    "        print(\"l2_rel_err(finite):\", diff.pow(2).sum().sqrt().item() / a[fin].pow(2).sum().sqrt().clamp_min(1e-12).item())\n",
    "        print(\"cosine_sim(finite):\", torch.nn.functional.cosine_similarity(a[fin].flatten(), b[fin].flatten(), dim=0).item())\n",
    "    else:\n",
    "        print(\"No overlapping finite values; values likely -inf everywhere except kept indices.\")\n",
    "\n",
    "    # Robust isclose on finite entries only\n",
    "    if fin.any():\n",
    "        frac_mismatch = (~torch.isclose(a[fin], b[fin], rtol=rtol, atol=atol)).float().mean().item()\n",
    "    else:\n",
    "        frac_mismatch = float('nan')\n",
    "    print(\"fraction_mismatch(isclose, finite):\", frac_mismatch)\n",
    "    # python\n",
    "    kidx_tf = torch.topk(x_tf, k).indices\n",
    "    kidx_fs = torch.topk(x_fs, k).indices\n",
    "    print(\"overlap:\", torch.isin(kidx_tf, kidx_fs).float().mean().item(),\n",
    "          \"kept_counts:\", torch.isfinite(x_tf).sum().item(), torch.isfinite(x_fs).sum().item())\n",
    "    assert torch.allclose(x_tf, x_fs, rtol=1e-3, atol=1e-3), \"will fail when _topk_filter_simple encounters rows with ties\""
   ],
   "id": "1bb8ce27cc92dcba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_abs_err(finite): 0.0\n",
      "mean_abs_err(finite): 0.0\n",
      "rel_err(finite): 0.0\n",
      "l2_rel_err(finite): 0.0\n",
      "cosine_sim(finite): 0.9999998807907104\n",
      "fraction_mismatch(isclose, finite): 0.0\n",
      "overlap: 1.0 kept_counts: 100 100\n",
      "max_abs_err(finite): 0.0\n",
      "mean_abs_err(finite): 0.0\n",
      "rel_err(finite): 0.0\n",
      "l2_rel_err(finite): 0.0\n",
      "cosine_sim(finite): 1.0\n",
      "fraction_mismatch(isclose, finite): 0.0\n",
      "overlap: 1.0 kept_counts: 100 100\n",
      "max_abs_err(finite): 0.0\n",
      "mean_abs_err(finite): 0.0\n",
      "rel_err(finite): 0.0\n",
      "l2_rel_err(finite): 0.0\n",
      "cosine_sim(finite): 1.0\n",
      "fraction_mismatch(isclose, finite): 0.0\n",
      "overlap: 1.0 kept_counts: 100 101\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[93]\u001B[39m\u001B[32m, line 53\u001B[39m\n\u001B[32m     50\u001B[39m kidx_fs = torch.topk(x_fs, k).indices\n\u001B[32m     51\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33moverlap:\u001B[39m\u001B[33m\"\u001B[39m, torch.isin(kidx_tf, kidx_fs).float().mean().item(),\n\u001B[32m     52\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mkept_counts:\u001B[39m\u001B[33m\"\u001B[39m, torch.isfinite(x_tf).sum().item(), torch.isfinite(x_fs).sum().item())\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m torch.allclose(x_tf, x_fs, rtol=\u001B[32m1e-3\u001B[39m, atol=\u001B[32m1e-3\u001B[39m)\n",
      "\u001B[31mAssertionError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T02:31:14.776390Z",
     "start_time": "2025-10-24T02:31:13.530902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prelude()\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "def _topp_filter(x, p):\n",
    "    if p is None or p <= 0.0 or p >= 1.0:\n",
    "        return x\n",
    "    probs = F.softmax(x, dim=-1)\n",
    "    s, i = torch.sort(probs, dim=-1, descending=True)\n",
    "    c = torch.cumsum(s, dim=-1)\n",
    "    ms = c <= p\n",
    "    ms[..., 0] = True\n",
    "    m = torch.zeros_like(ms, dtype=torch.bool).scatter(-1, i, ms)\n",
    "    return x.masked_fill(~m, float(\"-inf\"))\n",
    "\n",
    "def _topp_filter_simple(x, p):\n",
    "    if p is None or p <= 0.0 or p >= 1.0:\n",
    "        return x\n",
    "    probs = F.softmax(x, dim=-1)\n",
    "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "    cdf = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = cdf <= p\n",
    "    mask[..., 0] = True\n",
    "    keep = sorted_idx[mask]\n",
    "    new_x = torch.full_like(x, float(\"-inf\"))\n",
    "    new_x[keep] = x[keep]\n",
    "    return new_x\n",
    "\n",
    "dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "p = 0.9\n",
    "V = 50000\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "device = \"mps\"\n",
    "for dtype in dtypes:\n",
    "    X = torch.randn(V, dtype=dtype, device=device)\n",
    "    x_tf = _topp_filter(X, p)\n",
    "    x_fs = _topp_filter_simple(X, p)\n",
    "    print(f\"max val x_tf: {torch.argmax(x_tf, dim=-1)}\")\n",
    "    print(f\"max val x_fs: {torch.argmax(x_fs, dim=-1)}\")\n",
    "    assert torch.equal(x_tf, x_fs)"
   ],
   "id": "9f722f447593dc5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max val x_tf: 5822\n",
      "max val x_fs: 5822\n",
      "max val x_tf: 6912\n",
      "max val x_fs: 6912\n",
      "max val x_tf: 43770\n",
      "max val x_fs: 43770\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rotary DTypes",
   "id": "8b0502573dc9188b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T20:31:32.227773Z",
     "start_time": "2025-10-23T20:31:32.226435Z"
    }
   },
   "cell_type": "code",
   "source": "prelude()",
   "id": "e3e932d3e6f2686",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:08:39.679242Z",
     "start_time": "2025-10-23T22:08:39.676880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "def _apply_rope(x, cos, sin):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat((y1, y2), dim=-1)\n",
    "\n",
    "class RotaryPositionEncoding(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len, dtype):\n",
    "        super().__init__()\n",
    "        half = head_dim // 2\n",
    "        keep = head_dim // 4\n",
    "        base = 1024\n",
    "        if keep == 0:\n",
    "            angular = (1 / base) ** torch.linspace(0, 1, steps=half, dtype=dtype)\n",
    "        else:\n",
    "            active = (1 / base) ** torch.linspace(0, 1, steps=keep, dtype=dtype)\n",
    "            angular = torch.cat([active, active.new_zeros(half - keep)])\n",
    "        self.inv_freq = nn.Buffer(angular, persistent=False)\n",
    "        t = torch.arange(max_seq_len, dtype=dtype)\n",
    "        theta = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        self.cos = nn.Buffer(theta.cos().to(dtype), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin().to(dtype), persistent=False)\n",
    "        self._max_seq_len = int(max_seq_len)\n",
    "\n",
    "    def forward(self, x_BTHD):\n",
    "        L = x_BTHD.size(-3)\n",
    "        cos = self.cos[None, :L, None, :]\n",
    "        sin = self.sin[None, :L, None, :]\n",
    "        return _apply_rope(x_BTHD, cos, sin)\n",
    "\n"
   ],
   "id": "b27ca30b5492578",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:08:48.082200Z",
     "start_time": "2025-10-23T22:08:44.296683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import perf_counter as p\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"mps\"\n",
    "rotary_dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "ref_dtype = torch.float32\n",
    "B, T, H, D, iters, max_seq_len = 1, 1024, 8, 128, 2048, 65536\n",
    "ref_rotary = RotaryPositionEncoding(D, max_seq_len, dtype=ref_dtype).to(device)\n",
    "\n",
    "for dtype in rotary_dtypes:\n",
    "    rotary = RotaryPositionEncoding(D, max_seq_len, dtype=dtype).to(device)\n",
    "    X = torch.randn(B, T, H, D, dtype=dtype, device=device)\n",
    "    with dev_sync(device):\n",
    "        t0 = p()\n",
    "        for i in range(iters):\n",
    "            rotary(X)\n",
    "    dur = p() - t0\n",
    "    print(f\"Rotary [{dtype}] | Avg dur/iter: {dur/iters:.6f}s\")\n",
    "\n",
    "    S = torch.zeros(iters, dtype=ref_dtype, device=device)\n",
    "    for i in range(iters):\n",
    "        Y = torch.randn(B, T, H, D, dtype=ref_dtype, device=device)\n",
    "        approx = rotary(Y.to(dtype)).to(ref_dtype)\n",
    "        ref_res = ref_rotary(Y)\n",
    "        dist = F.cosine_similarity(ref_res, approx, dim=-1).mean()\n",
    "        S[i] = dist\n",
    "    print(f\"Rotary [{dtype}] | Mean cosine similarity: {S.mean().item():.6f}\")\n"
   ],
   "id": "5549b74b14801255",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotary [torch.float32] | Avg dur/iter: 0.000150s\n",
      "Rotary [torch.float32] | Mean cosine similarity: 1.000000\n",
      "Rotary [torch.float16] | Avg dur/iter: 0.000136s\n",
      "Rotary [torch.float16] | Mean cosine similarity: 0.999386\n",
      "Rotary [torch.bfloat16] | Avg dur/iter: 0.000137s\n",
      "Rotary [torch.bfloat16] | Mean cosine similarity: 0.959024\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
