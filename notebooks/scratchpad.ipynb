{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:53.845715Z",
     "start_time": "2025-11-03T21:00:53.150663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([ 512, 768, 1024, 1280, 1600])\n",
    "B = (A/768) ** -0.5\n",
    "# Python\n",
    "C = {int(a): float(b) for a, b in zip(A.tolist(), B.tolist())}\n",
    "print(f\"dim by lr scale ratio: {C}\")"
   ],
   "id": "738999f593e1421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim by lr scale ratio: {512: 1.2247447967529297, 768: 1.0, 1024: 0.866025447845459, 1280: 0.7745966911315918, 1600: 0.6928203701972961}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:53.878693Z",
     "start_time": "2025-11-03T21:00:53.860400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from daisy.daisy_core import next_multiple_of_n\n",
    "\n",
    "def estimate_params(L: int, d: int, V: int, tied_head: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict with per-layer, embeddings, and total params (in integers and millions).\n",
    "    Formula:\n",
    "      - Attention (Q,K,V,O): 4 * d * d\n",
    "      - d_fc: 4 * d\n",
    "      - MLP: 2 * d * d_fc\n",
    "      - Per-layer ≈ 4d^2 + 3 d d_fc  (defaults to 16 d^2 when d_fc=4d)\n",
    "      - Embeddings: V * d\n",
    "      - Output head: + V * d if untied\n",
    "    \"\"\"\n",
    "    d_fc = 4 * d\n",
    "    mlp = 2 * d * d_fc\n",
    "    attn = 4 * d * d\n",
    "    per_layer = mlp + attn\n",
    "    layers_total = L * per_layer\n",
    "    embed = V * d\n",
    "    head = 0 if tied_head else next_multiple_of_n(V, n=128) * d\n",
    "    ve = 3 * V * d\n",
    "    scalars = 5 * L\n",
    "    total = layers_total + embed + head + ve + scalars\n",
    "    to_m = lambda x: round(x / 1e6, 3)\n",
    "    return {\n",
    "        \"per_layer\": per_layer,\n",
    "        \"per_layer_M\": to_m(per_layer),\n",
    "        \"layers_total\": layers_total,\n",
    "        \"layers_total_M\": to_m(layers_total),\n",
    "        \"embeddings\": embed,\n",
    "        \"embeddings_M\": to_m(embed),\n",
    "        \"output_head\": head,\n",
    "        \"output_head_M\": to_m(head),\n",
    "        \"value_embeddings\": ve,\n",
    "        \"value_embeddings_M\": to_m(ve),\n",
    "        \"total\": total,\n",
    "        \"total_M\": to_m(total),\n",
    "    }"
   ],
   "id": "70f28c3dc5ac3c1b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:54.733557Z",
     "start_time": "2025-11-03T21:00:53.884659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from training.hparams import load_hparams_from_yaml\n",
    "hparams = load_hparams_from_yaml(\"../config/pretrain_pico.yml\")\n",
    "L = hparams.num_layers\n",
    "d = hparams.model_dim\n",
    "V = hparams.vocab_size\n",
    "res = estimate_params(L=L, d=d, V=V, tied_head=False)\n",
    "print(json.dumps(res, indent=2, sort_keys=True))"
   ],
   "id": "df1f7c86c06b686f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"embeddings\": 25731584,\n",
      "  \"embeddings_M\": 25.732,\n",
      "  \"layers_total\": 18874368,\n",
      "  \"layers_total_M\": 18.874,\n",
      "  \"output_head\": 25755648,\n",
      "  \"output_head_M\": 25.756,\n",
      "  \"per_layer\": 3145728,\n",
      "  \"per_layer_M\": 3.146,\n",
      "  \"total\": 147556382,\n",
      "  \"total_M\": 147.556,\n",
      "  \"value_embeddings\": 77194752,\n",
      "  \"value_embeddings_M\": 77.195\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:54.761427Z",
     "start_time": "2025-11-03T21:00:54.759483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_param_data_ratio = 20\n",
    "num_params = 350\n",
    "target_tokens = target_param_data_ratio * num_params\n",
    "print(f\"Target tokens (M): {target_tokens}\")"
   ],
   "id": "8c461d4b295c1d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tokens (M): 7000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:54.780880Z",
     "start_time": "2025-11-03T21:00:54.779139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scale = 0.866025447845459\n",
    "head_lr = 0.004 * scale\n",
    "embed_lr = 0.2 * scale\n",
    "scalar_lr = 0.015 * scale\n",
    "\n",
    "print(f\"Head LR: {head_lr}\")\n",
    "print(f\"Embed LR: {embed_lr}\")\n",
    "print(f\"Scalar LR: {scalar_lr}\")\n"
   ],
   "id": "18b1710a80f2eae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head LR: 0.003464101791381836\n",
      "Embed LR: 0.1732050895690918\n",
      "Scalar LR: 0.012990381717681885\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:54.791361Z",
     "start_time": "2025-11-03T21:00:54.789661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_scale = 1/50\n",
    "print(f\"Head LR: {head_lr*sft_scale}\")\n",
    "print(f\"Embed LR: {embed_lr*sft_scale}\")\n",
    "print(f\"Scalar LR: {scalar_lr*sft_scale}\")"
   ],
   "id": "5c6607943869af84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head LR: 6.928203582763672e-05\n",
      "Embed LR: 0.003464101791381836\n",
      "Scalar LR: 0.0002598076343536377\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:54.805830Z",
     "start_time": "2025-11-03T21:00:54.802503Z"
    }
   },
   "cell_type": "code",
   "source": "0.015 * sft_scale",
   "id": "de17d1459ebc7468",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:55.083821Z",
     "start_time": "2025-11-03T21:00:54.820556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib.ticker import PercentFormatter, MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "def plot_param_heatmaps(model, cols=6, cmap_count=\"viridis\", cmap_bytes=\"magma\"):\n",
    "    items = []\n",
    "    for name, t in model.state_dict().items():\n",
    "        n = t.numel()\n",
    "        nb = n * t.element_size()\n",
    "        # capture dtype suffix without \"torch.\"\n",
    "        dt = str(t.dtype).replace(\"torch.\", \"\")\n",
    "        items.append((name, n, nb, dt))\n",
    "\n",
    "    # Sort consistently to keep positions aligned across both maps\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    names = [x[0] for x in items]\n",
    "    dtypes = [x[3] for x in items]\n",
    "    counts = np.array([x[1] for x in items], dtype=float)\n",
    "    bytes_ = np.array([x[2] for x in items], dtype=float)\n",
    "\n",
    "    total_counts = counts.sum() if counts.sum() > 0 else 1.0\n",
    "    total_bytes = bytes_.sum() if bytes_.sum() > 0 else 1.0\n",
    "    frac_counts = counts / total_counts\n",
    "    frac_bytes = bytes_ / total_bytes\n",
    "\n",
    "    n = len(items)\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    size = rows * cols\n",
    "\n",
    "    grid_counts = np.zeros(size, dtype=float)\n",
    "    grid_bytes = np.zeros(size, dtype=float)\n",
    "    grid_counts[:n] = frac_counts\n",
    "    grid_bytes[:n] = frac_bytes\n",
    "    grid_counts = grid_counts.reshape(rows, cols)\n",
    "    grid_bytes = grid_bytes.reshape(rows, cols)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(cols*1.4*2, max(1.6, rows*1.4)))\n",
    "    # by count\n",
    "    im1 = axes[0].imshow(grid_counts, cmap=cmap_count, aspect=\"auto\")\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], label=\"Share of total params\")\n",
    "    cbar1.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))  # 0–100%\n",
    "    cbar1.ax.yaxis.set_major_locator(MaxNLocator(nbins=6, prune=None))\n",
    "\n",
    "    max1 = grid_counts.max() if grid_counts.size else 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i*cols + j\n",
    "            if idx < n:\n",
    "                label = f\"{names[idx].split('.')[-1]}\\n{dtypes[idx]}\\n{frac_counts[idx]*100:.1f}%\"\n",
    "                color = \"white\" if grid_counts[i, j] > (max1/3 if max1 else 0) else \"black\"\n",
    "                axes[0].text(j, i, label, ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "    axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "    axes[0].set_title(\"Parameter distribution (count)\")\n",
    "\n",
    "    # norm = colors.Normalize(vmin=0, vmax=0.2)\n",
    "    #  gamma-like emphasis\n",
    "    norm = colors.PowerNorm(gamma=0.5)                  # brighten small values\n",
    "    # percentile-based clipping\n",
    "    def pct_norm(a, lo=2, hi=98):\n",
    "        vmin, vmax = np.percentile(a[a>0], [lo, hi]) if (a>0).any() else (0, 1)\n",
    "        return colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # im = axes[0].imshow(grid_counts, cmap=cmap_count, norm=norm, aspect=\"auto\")\n",
    "\n",
    "    # by bytes\n",
    "    im2 = axes[1].imshow(grid_bytes, cmap=cmap_bytes, aspect=\"auto\", norm=norm)\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], label=\"Share of total bytes\")\n",
    "    cbar2.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))\n",
    "    cbar2.ax.yaxis.set_major_locator(MaxNLocator(nbins=6, prune=None))\n",
    "    max2 = grid_bytes.max() if grid_bytes.size else 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i*cols + j\n",
    "            if idx < n:\n",
    "                label = f\"{names[idx].split('.')[-1]}\\n{dtypes[idx]}\\n{frac_bytes[idx]*100:.1f}%\"\n",
    "                color = \"white\" if grid_bytes[i, j] > (max2/3 if max2 else 0) else \"black\"\n",
    "                axes[1].text(j, i, label, ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "    axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "    axes[1].set_title(f\"Parameter distribution (bytes) ~ {total_bytes/1e6:.1f} MB\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d5b97b68b44b7dbd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:55.093167Z",
     "start_time": "2025-11-03T21:00:55.089811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "def p_stats(model:nn.Module):\n",
    "\n",
    "    def format_dtype(dt: torch.dtype) -> str:\n",
    "        return str(dt).replace(\"torch.\", \"\")\n",
    "\n",
    "    def format_shape(t: torch.Tensor) -> str:\n",
    "        return str(list(t.shape))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    header = [\n",
    "        (\"name\", 40),\n",
    "        (\"shape\", 24),\n",
    "        (\"dtype\", 10),\n",
    "        (\"numel\", 14),\n",
    "        (\"%total\", 8),\n",
    "    ]\n",
    "    line_fmt = f\"{{name:<{header[0][1]}}}  {{shape:<{header[1][1]}}}  {{dtype:<{header[2][1]}}}  {{numel:>{header[3][1]}}}  {{pct:>{header[4][1]}}}\"\n",
    "\n",
    "    print(line_fmt.format(\n",
    "        name=header[0][0], shape=header[1][0], dtype=header[2][0], numel=header[3][0], pct=header[4][0]\n",
    "    ))\n",
    "\n",
    "    for name, t in model.state_dict().items():\n",
    "        print(line_fmt.format(\n",
    "            name=name,\n",
    "            shape=format_shape(t),\n",
    "            dtype=format_dtype(t.dtype),\n",
    "            numel=f\"{t.numel():,}\",\n",
    "            pct=f\"{(t.numel()/total_params)*100:.2f}%\"\n",
    "    ))"
   ],
   "id": "abc3d0b68168f442",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:55.250718Z",
     "start_time": "2025-11-03T21:00:55.098315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from checkpoint import model_from_checkpoint\n",
    "model, _ = model_from_checkpoint(\"../checkpoints/20251103T0243-val5.810-step000600-run0-best.pt\", device='mps')\n",
    "model.eval()"
   ],
   "id": "cd14eeab95788509",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/20251103T0243-val5.810-step000600-run0-best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcheckpoint\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m model_from_checkpoint\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model, _ = \u001B[43mmodel_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m../checkpoints/20251103T0243-val5.810-step000600-run0-best.pt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmps\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy-wee/tools/checkpoint.py:126\u001B[39m, in \u001B[36mmodel_from_checkpoint\u001B[39m\u001B[34m(path, device)\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmodel_from_checkpoint\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m, device: torch.device | \u001B[38;5;28mstr\u001B[39m) -> nn.Module:\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m     ckpt = \u001B[43mload_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m     state_dict = ckpt.model\n\u001B[32m    128\u001B[39m     hparams = ckpt.hparams \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy-wee/tools/checkpoint.py:77\u001B[39m, in \u001B[36mload_checkpoint\u001B[39m\u001B[34m(path, map_location, strip_prefix)\u001B[39m\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_checkpoint\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m, map_location: Any | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m, strip_prefix: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m) -> LoadedCheckpoint:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     obj = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     78\u001B[39m     ckpt = _normalize(obj)\n\u001B[32m     79\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m strip_prefix:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/serialization.py:1500\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1497\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1498\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1500\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1501\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1502\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1503\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1504\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1505\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/serialization.py:768\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    766\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer: FileLike, mode: \u001B[38;5;28mstr\u001B[39m) -> _opener[IO[\u001B[38;5;28mbytes\u001B[39m]]:\n\u001B[32m    767\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m768\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    769\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    770\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/serialization.py:749\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    748\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike[\u001B[38;5;28mstr\u001B[39m]], mode: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '../checkpoints/20251103T0243-val5.810-step000600-run0-best.pt'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:00:55.269227Z",
     "start_time": "2025-11-03T20:49:45.201711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import model_from_spec\n",
    "from tools.model_report import build_report, format_report_text\n",
    "\n",
    "model = model_from_spec(\"daisy_pico\", device='mps')\n",
    "report = build_report(model)\n",
    "print(format_report_text(report))"
   ],
   "id": "2b2d3b7892269531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checkpoint ===\n",
      "\n",
      "=== Model stats ===\n",
      "parameters (total): 70,361,630 (70361630)\n",
      "parameters (trainable): 70,361,630 (70361630)\n",
      "parameter size: 232.41 MiB\n",
      "model type: DaisyCore\n",
      "layers: 6\n",
      "\n",
      "parameter dtypes:\n",
      "  float32: 51,487,262 (51487262)\n",
      "  bfloat16: 18,874,368 (18874368)\n",
      "\n",
      "=== Learned scalars (DaisyCore) ===\n",
      "num_layers (inferred): 6\n",
      "threshold for near-zero: 0.001\n",
      "- skip_weights: shape=[6], min=1, max=1, mean=1, std=0\n",
      "  near-zero: 0 elements (0.00%)\n",
      "- lambdas: shape=[6, 2], min=0, max=1, mean=0.5, std=0.5\n",
      "  near-zero: 6 elements (50.00%)\n",
      "- sa_lambdas: shape=[6, 2], min=0.5, max=0.5, mean=0.5, std=0\n",
      "  near-zero: 0 elements (0.00%)\n",
      "\n",
      "Per-layer (i: skip | lambda -> sa_lambda):\n",
      "  00: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  01: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  02: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  03: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  04: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  05: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "\n",
      "Note: values marked with * are near zero and may indicate unused pathways.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:01:48.164203Z",
     "start_time": "2025-11-03T21:01:48.148765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "sd = model.state_dict()\n",
    "def storage_key(t):\n",
    "    s = t.untyped_storage()\n",
    "    return (s.data_ptr(), s.nbytes())\n",
    "\n",
    "groups = {}\n",
    "for k, v in sd.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        groups.setdefault(storage_key(v), []).append(k)\n",
    "\n",
    "print(\"## Shared Storage:\")\n",
    "for k, names in groups.items():\n",
    "    if len(names) > 1:\n",
    "        print(names)\n",
    "\n",
    "print(\"\\n## Unique Storage:\")\n",
    "for k, names in groups.items():\n",
    "    if len(names) == 1:\n",
    "        print(names)"
   ],
   "id": "68a48e821ad7b88c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m sd = \u001B[43mmodel\u001B[49m.state_dict()\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstorage_key\u001B[39m(t):\n\u001B[32m      5\u001B[39m     s = t.untyped_storage()\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:31:19.651020Z",
     "start_time": "2025-11-03T21:31:18.423193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, zipfile, os\n",
    "\n",
    "def inspect_sd(sd):\n",
    "    by_dtype = {}\n",
    "    total_bytes = 0\n",
    "    for k, v in sd.items():\n",
    "        if 'value' in k:\n",
    "            print(k,v)\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            nbytes = v.untyped_storage().nbytes()\n",
    "            total_bytes += nbytes\n",
    "            by_dtype.setdefault(str(v.dtype), 0)\n",
    "            by_dtype[str(v.dtype)] += nbytes\n",
    "    print(\"bytes by dtype:\", {k: f\"{v/1024**2:.2f} MiB\" for k,v in by_dtype.items()})\n",
    "    print(\"sum(storage nbytes):\", f\"{total_bytes/1024**2:.2f} MiB\")\n",
    "\n",
    "\n",
    "# with zipfile.ZipFile(path, \"r\") as z:\n",
    "#     stored = [i.file_size for i in z.infolist() if '/data/' in i.filename]\n",
    "# print(\"sum(zip data/*):\", f\"{sum(stored)/1024**2:.2f} MiB\", \"entries:\", len(stored))\n",
    "\n",
    "from checkpoint import model_from_checkpoint\n",
    "\n",
    "path = \"../checkpoints/20251103T2120-val5.780-step000101-run0-final.pt\"  # your file\n",
    "pt_sd = torch.load(path, weights_only=True, mmap=True)['model']\n",
    "pt_model, hparams = model_from_checkpoint(path, device='mps')\n",
    "\n",
    "inspect_sd(pt_sd)\n",
    "inspect_sd(pt_model.state_dict())\n"
   ],
   "id": "f0de43a5cf8c41f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_embeds.0.weight tensor([[ 0.4043,  3.7188, -3.3281,  ..., -1.9922, -1.2031, -1.6875],\n",
      "        [-0.1475,  1.3594, -0.7344,  ..., -2.0781, -0.4492, -2.5938],\n",
      "        [-4.5625, -1.3594, -2.7031,  ..., -1.8359,  2.7656,  1.1406],\n",
      "        ...,\n",
      "        [-0.2051, -2.9062,  1.2578,  ..., -1.2969, -1.9453,  0.8047],\n",
      "        [ 3.7656, -0.0649, -0.7500,  ...,  0.2812,  1.1406,  1.2969],\n",
      "        [-0.8828, -2.5000,  0.1245,  ..., -0.2129, -0.0688,  5.3750]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.1.weight tensor([[-0.8867,  1.6797, -3.6406,  ..., -0.1001,  2.1875, -3.1875],\n",
      "        [ 0.4961,  0.0591, -0.3613,  ..., -2.6875, -0.9570, -2.3750],\n",
      "        [ 1.5781,  4.4062,  0.6523,  ..., -1.7969,  0.9922, -0.8164],\n",
      "        ...,\n",
      "        [-0.2148,  1.7969,  0.9297,  ..., -1.1016,  1.6875,  0.8438],\n",
      "        [-0.5898,  0.4082, -1.0547,  ...,  1.0859,  0.3711,  0.5039],\n",
      "        [ 3.1406,  1.5469,  0.8164,  ..., -1.2031, -2.1250, -0.3438]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.2.weight tensor([[-2.5625e+00, -5.0781e-01, -2.0938e+00,  ...,  2.1719e+00,\n",
      "          2.2188e+00, -3.4424e-02],\n",
      "        [ 1.9844e+00,  1.2734e+00, -2.1875e+00,  ..., -1.9766e+00,\n",
      "         -1.9531e+00, -6.2500e-01],\n",
      "        [ 1.5547e+00, -2.3281e+00, -2.8750e+00,  ...,  9.6130e-04,\n",
      "         -7.2754e-02, -2.9102e-01],\n",
      "        ...,\n",
      "        [-1.2578e+00, -1.1562e+00, -6.3281e-01,  ...,  5.1953e-01,\n",
      "         -3.8867e-01, -8.9453e-01],\n",
      "        [-1.7031e+00, -2.5938e+00, -5.2344e-01,  ..., -2.0156e+00,\n",
      "         -3.0625e+00,  4.1406e-01],\n",
      "        [ 5.0391e-01,  7.2656e-01,  6.3477e-02,  ..., -1.1562e+00,\n",
      "          1.4609e+00,  4.4336e-01]], device='mps:0', dtype=torch.bfloat16)\n",
      "bytes by dtype: {'torch.float32': '98.25 MiB', 'torch.bfloat16': '232.32 MiB'}\n",
      "sum(storage nbytes): 330.57 MiB\n",
      "value_embeds.0.weight tensor([[ 0.4043,  3.7188, -3.3281,  ..., -1.9922, -1.2031, -1.6875],\n",
      "        [-0.1475,  1.3594, -0.7344,  ..., -2.0781, -0.4492, -2.5938],\n",
      "        [-4.5625, -1.3594, -2.7031,  ..., -1.8359,  2.7656,  1.1406],\n",
      "        ...,\n",
      "        [-0.2051, -2.9062,  1.2578,  ..., -1.2969, -1.9453,  0.8047],\n",
      "        [ 3.7656, -0.0649, -0.7500,  ...,  0.2812,  1.1406,  1.2969],\n",
      "        [-0.8828, -2.5000,  0.1245,  ..., -0.2129, -0.0688,  5.3750]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.1.weight tensor([[-0.8867,  1.6797, -3.6406,  ..., -0.1001,  2.1875, -3.1875],\n",
      "        [ 0.4961,  0.0591, -0.3613,  ..., -2.6875, -0.9570, -2.3750],\n",
      "        [ 1.5781,  4.4062,  0.6523,  ..., -1.7969,  0.9922, -0.8164],\n",
      "        ...,\n",
      "        [-0.2148,  1.7969,  0.9297,  ..., -1.1016,  1.6875,  0.8438],\n",
      "        [-0.5898,  0.4082, -1.0547,  ...,  1.0859,  0.3711,  0.5039],\n",
      "        [ 3.1406,  1.5469,  0.8164,  ..., -1.2031, -2.1250, -0.3438]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.2.weight tensor([[-2.5625e+00, -5.0781e-01, -2.0938e+00,  ...,  2.1719e+00,\n",
      "          2.2188e+00, -3.4424e-02],\n",
      "        [ 1.9844e+00,  1.2734e+00, -2.1875e+00,  ..., -1.9766e+00,\n",
      "         -1.9531e+00, -6.2500e-01],\n",
      "        [ 1.5547e+00, -2.3281e+00, -2.8750e+00,  ...,  9.6130e-04,\n",
      "         -7.2754e-02, -2.9102e-01],\n",
      "        ...,\n",
      "        [-1.2578e+00, -1.1562e+00, -6.3281e-01,  ...,  5.1953e-01,\n",
      "         -3.8867e-01, -8.9453e-01],\n",
      "        [-1.7031e+00, -2.5938e+00, -5.2344e-01,  ..., -2.0156e+00,\n",
      "         -3.0625e+00,  4.1406e-01],\n",
      "        [ 5.0391e-01,  7.2656e-01,  6.3477e-02,  ..., -1.1562e+00,\n",
      "          1.4609e+00,  4.4336e-01]], device='mps:0', dtype=torch.bfloat16)\n",
      "bytes by dtype: {'torch.float32': '98.25 MiB', 'torch.bfloat16': '232.32 MiB'}\n",
      "sum(storage nbytes): 330.57 MiB\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
