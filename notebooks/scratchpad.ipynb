{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:58.925881Z",
     "start_time": "2025-11-03T21:34:58.242270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([ 512, 768, 1024, 1280, 1600])\n",
    "B = (A/768) ** -0.5\n",
    "# Python\n",
    "C = {int(a): float(b) for a, b in zip(A.tolist(), B.tolist())}\n",
    "print(f\"dim by lr scale ratio: {C}\")"
   ],
   "id": "738999f593e1421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim by lr scale ratio: {512: 1.2247447967529297, 768: 1.0, 1024: 0.866025447845459, 1280: 0.7745966911315918, 1600: 0.6928203701972961}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:58.943897Z",
     "start_time": "2025-11-03T21:34:58.930517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from daisy.daisy_core import next_multiple_of_n\n",
    "\n",
    "def estimate_params(L: int, d: int, V: int, tied_head: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict with per-layer, embeddings, and total params (in integers and millions).\n",
    "    Formula:\n",
    "      - Attention (Q,K,V,O): 4 * d * d\n",
    "      - d_fc: 4 * d\n",
    "      - MLP: 2 * d * d_fc\n",
    "      - Per-layer ≈ 4d^2 + 3 d d_fc  (defaults to 16 d^2 when d_fc=4d)\n",
    "      - Embeddings: V * d\n",
    "      - Output head: + V * d if untied\n",
    "    \"\"\"\n",
    "    d_fc = 4 * d\n",
    "    mlp = 2 * d * d_fc\n",
    "    attn = 4 * d * d\n",
    "    per_layer = mlp + attn\n",
    "    layers_total = L * per_layer\n",
    "    embed = V * d\n",
    "    head = 0 if tied_head else next_multiple_of_n(V, n=128) * d\n",
    "    ve = 3 * V * d\n",
    "    scalars = 5 * L\n",
    "    total = layers_total + embed + head + ve + scalars\n",
    "    to_m = lambda x: round(x / 1e6, 3)\n",
    "    return {\n",
    "        \"per_layer\": per_layer,\n",
    "        \"per_layer_M\": to_m(per_layer),\n",
    "        \"layers_total\": layers_total,\n",
    "        \"layers_total_M\": to_m(layers_total),\n",
    "        \"embeddings\": embed,\n",
    "        \"embeddings_M\": to_m(embed),\n",
    "        \"output_head\": head,\n",
    "        \"output_head_M\": to_m(head),\n",
    "        \"value_embeddings\": ve,\n",
    "        \"value_embeddings_M\": to_m(ve),\n",
    "        \"total\": total,\n",
    "        \"total_M\": to_m(total),\n",
    "    }"
   ],
   "id": "70f28c3dc5ac3c1b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:59.762899Z",
     "start_time": "2025-11-03T21:34:58.989786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from training.hparams import load_hparams_from_yaml\n",
    "hparams = load_hparams_from_yaml(\"../config/pretrain_pico.yml\")\n",
    "L = hparams.num_layers\n",
    "d = hparams.model_dim\n",
    "V = hparams.vocab_size\n",
    "res = estimate_params(L=L, d=d, V=V, tied_head=False)\n",
    "print(json.dumps(res, indent=2, sort_keys=True))"
   ],
   "id": "df1f7c86c06b686f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"embeddings\": 25731584,\n",
      "  \"embeddings_M\": 25.732,\n",
      "  \"layers_total\": 18874368,\n",
      "  \"layers_total_M\": 18.874,\n",
      "  \"output_head\": 25755648,\n",
      "  \"output_head_M\": 25.756,\n",
      "  \"per_layer\": 3145728,\n",
      "  \"per_layer_M\": 3.146,\n",
      "  \"total\": 147556382,\n",
      "  \"total_M\": 147.556,\n",
      "  \"value_embeddings\": 77194752,\n",
      "  \"value_embeddings_M\": 77.195\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:59.771383Z",
     "start_time": "2025-11-03T21:34:59.769462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_param_data_ratio = 20\n",
    "num_params = 350\n",
    "target_tokens = target_param_data_ratio * num_params\n",
    "print(f\"Target tokens (M): {target_tokens}\")"
   ],
   "id": "8c461d4b295c1d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tokens (M): 7000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:59.789384Z",
     "start_time": "2025-11-03T21:34:59.787635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scale = 0.866025447845459\n",
    "head_lr = 0.004 * scale\n",
    "embed_lr = 0.2 * scale\n",
    "scalar_lr = 0.015 * scale\n",
    "\n",
    "print(f\"Head LR: {head_lr}\")\n",
    "print(f\"Embed LR: {embed_lr}\")\n",
    "print(f\"Scalar LR: {scalar_lr}\")\n"
   ],
   "id": "18b1710a80f2eae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head LR: 0.003464101791381836\n",
      "Embed LR: 0.1732050895690918\n",
      "Scalar LR: 0.012990381717681885\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:59.800059Z",
     "start_time": "2025-11-03T21:34:59.798305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_scale = 1/50\n",
    "print(f\"Head LR: {head_lr*sft_scale}\")\n",
    "print(f\"Embed LR: {embed_lr*sft_scale}\")\n",
    "print(f\"Scalar LR: {scalar_lr*sft_scale}\")"
   ],
   "id": "5c6607943869af84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head LR: 6.928203582763672e-05\n",
      "Embed LR: 0.003464101791381836\n",
      "Scalar LR: 0.0002598076343536377\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:34:59.822554Z",
     "start_time": "2025-11-03T21:34:59.819174Z"
    }
   },
   "cell_type": "code",
   "source": "0.015 * sft_scale",
   "id": "de17d1459ebc7468",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:35:00.110168Z",
     "start_time": "2025-11-03T21:34:59.840805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib.ticker import PercentFormatter, MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "def plot_param_heatmaps(model, cols=6, cmap_count=\"viridis\", cmap_bytes=\"magma\"):\n",
    "    items = []\n",
    "    for name, t in model.state_dict().items():\n",
    "        n = t.numel()\n",
    "        nb = n * t.element_size()\n",
    "        # capture dtype suffix without \"torch.\"\n",
    "        dt = str(t.dtype).replace(\"torch.\", \"\")\n",
    "        items.append((name, n, nb, dt))\n",
    "\n",
    "    # Sort consistently to keep positions aligned across both maps\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    names = [x[0] for x in items]\n",
    "    dtypes = [x[3] for x in items]\n",
    "    counts = np.array([x[1] for x in items], dtype=float)\n",
    "    bytes_ = np.array([x[2] for x in items], dtype=float)\n",
    "\n",
    "    total_counts = counts.sum() if counts.sum() > 0 else 1.0\n",
    "    total_bytes = bytes_.sum() if bytes_.sum() > 0 else 1.0\n",
    "    frac_counts = counts / total_counts\n",
    "    frac_bytes = bytes_ / total_bytes\n",
    "\n",
    "    n = len(items)\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    size = rows * cols\n",
    "\n",
    "    grid_counts = np.zeros(size, dtype=float)\n",
    "    grid_bytes = np.zeros(size, dtype=float)\n",
    "    grid_counts[:n] = frac_counts\n",
    "    grid_bytes[:n] = frac_bytes\n",
    "    grid_counts = grid_counts.reshape(rows, cols)\n",
    "    grid_bytes = grid_bytes.reshape(rows, cols)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(cols*1.4*2, max(1.6, rows*1.4)))\n",
    "    # by count\n",
    "    im1 = axes[0].imshow(grid_counts, cmap=cmap_count, aspect=\"auto\")\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], label=\"Share of total params\")\n",
    "    cbar1.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))  # 0–100%\n",
    "    cbar1.ax.yaxis.set_major_locator(MaxNLocator(nbins=6, prune=None))\n",
    "\n",
    "    max1 = grid_counts.max() if grid_counts.size else 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i*cols + j\n",
    "            if idx < n:\n",
    "                label = f\"{names[idx].split('.')[-1]}\\n{dtypes[idx]}\\n{frac_counts[idx]*100:.1f}%\"\n",
    "                color = \"white\" if grid_counts[i, j] > (max1/3 if max1 else 0) else \"black\"\n",
    "                axes[0].text(j, i, label, ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "    axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "    axes[0].set_title(\"Parameter distribution (count)\")\n",
    "\n",
    "    # norm = colors.Normalize(vmin=0, vmax=0.2)\n",
    "    #  gamma-like emphasis\n",
    "    norm = colors.PowerNorm(gamma=0.5)                  # brighten small values\n",
    "    # percentile-based clipping\n",
    "    def pct_norm(a, lo=2, hi=98):\n",
    "        vmin, vmax = np.percentile(a[a>0], [lo, hi]) if (a>0).any() else (0, 1)\n",
    "        return colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # im = axes[0].imshow(grid_counts, cmap=cmap_count, norm=norm, aspect=\"auto\")\n",
    "\n",
    "    # by bytes\n",
    "    im2 = axes[1].imshow(grid_bytes, cmap=cmap_bytes, aspect=\"auto\", norm=norm)\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], label=\"Share of total bytes\")\n",
    "    cbar2.ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))\n",
    "    cbar2.ax.yaxis.set_major_locator(MaxNLocator(nbins=6, prune=None))\n",
    "    max2 = grid_bytes.max() if grid_bytes.size else 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i*cols + j\n",
    "            if idx < n:\n",
    "                label = f\"{names[idx].split('.')[-1]}\\n{dtypes[idx]}\\n{frac_bytes[idx]*100:.1f}%\"\n",
    "                color = \"white\" if grid_bytes[i, j] > (max2/3 if max2 else 0) else \"black\"\n",
    "                axes[1].text(j, i, label, ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "    axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "    axes[1].set_title(f\"Parameter distribution (bytes) ~ {total_bytes/1e6:.1f} MB\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d5b97b68b44b7dbd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:35:00.119788Z",
     "start_time": "2025-11-03T21:35:00.116387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "def p_stats(model:nn.Module):\n",
    "\n",
    "    def format_dtype(dt: torch.dtype) -> str:\n",
    "        return str(dt).replace(\"torch.\", \"\")\n",
    "\n",
    "    def format_shape(t: torch.Tensor) -> str:\n",
    "        return str(list(t.shape))\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    header = [\n",
    "        (\"name\", 40),\n",
    "        (\"shape\", 24),\n",
    "        (\"dtype\", 10),\n",
    "        (\"numel\", 14),\n",
    "        (\"%total\", 8),\n",
    "    ]\n",
    "    line_fmt = f\"{{name:<{header[0][1]}}}  {{shape:<{header[1][1]}}}  {{dtype:<{header[2][1]}}}  {{numel:>{header[3][1]}}}  {{pct:>{header[4][1]}}}\"\n",
    "\n",
    "    print(line_fmt.format(\n",
    "        name=header[0][0], shape=header[1][0], dtype=header[2][0], numel=header[3][0], pct=header[4][0]\n",
    "    ))\n",
    "\n",
    "    for name, t in model.state_dict().items():\n",
    "        print(line_fmt.format(\n",
    "            name=name,\n",
    "            shape=format_shape(t),\n",
    "            dtype=format_dtype(t.dtype),\n",
    "            numel=f\"{t.numel():,}\",\n",
    "            pct=f\"{(t.numel()/total_params)*100:.2f}%\"\n",
    "    ))"
   ],
   "id": "abc3d0b68168f442",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:35:48.968414Z",
     "start_time": "2025-11-03T21:35:47.566188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from checkpoint import model_from_checkpoint\n",
    "model, _ = model_from_checkpoint(\"../checkpoints/20251103T2120-val5.780-step000101-run0-final.pt\", device='mps')\n",
    "model.eval()"
   ],
   "id": "cd14eeab95788509",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DaisyCore(\n",
       "  (embed): Embedding(50257, 512)\n",
       "  (value_embeds): ModuleList(\n",
       "    (0-2): 3 x Embedding(50257, 512)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:35:51.453086Z",
     "start_time": "2025-11-03T21:35:51.042682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import model_from_spec\n",
    "from tools.model_report import build_report, format_report_text\n",
    "\n",
    "model = model_from_spec(\"daisy_pico\", device='mps')\n",
    "report = build_report(model)\n",
    "print(format_report_text(report))"
   ],
   "id": "2b2d3b7892269531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checkpoint ===\n",
      "\n",
      "=== Model stats ===\n",
      "parameters (total): 70,361,630 (70361630)\n",
      "parameters (trainable): 70,361,630 (70361630)\n",
      "parameter size: 232.41 MiB\n",
      "model type: DaisyCore\n",
      "layers: 6\n",
      "\n",
      "parameter dtypes:\n",
      "  float32: 51,487,262 (51487262)\n",
      "  bfloat16: 18,874,368 (18874368)\n",
      "\n",
      "=== Learned scalars (DaisyCore) ===\n",
      "num_layers (inferred): 6\n",
      "threshold for near-zero: 0.001\n",
      "- skip_weights: shape=[6], min=1, max=1, mean=1, std=0\n",
      "  near-zero: 0 elements (0.00%)\n",
      "- lambdas: shape=[6, 2], min=0, max=1, mean=0.5, std=0.5\n",
      "  near-zero: 6 elements (50.00%)\n",
      "- sa_lambdas: shape=[6, 2], min=0.5, max=0.5, mean=0.5, std=0\n",
      "  near-zero: 0 elements (0.00%)\n",
      "\n",
      "Per-layer (i: skip | lambda -> sa_lambda):\n",
      "  00: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  01: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  02: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  03: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  04: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "  05: 1.0000 | [1.0000, 0.0000*] -> [0.5000, 0.5000]\n",
      "\n",
      "Note: values marked with * are near zero and may indicate unused pathways.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:36:11.874457Z",
     "start_time": "2025-11-03T21:36:11.871539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "sd = model.state_dict()\n",
    "def storage_key(t):\n",
    "    s = t.untyped_storage()\n",
    "    return (s.data_ptr(), s.nbytes())\n",
    "\n",
    "groups = {}\n",
    "for k, v in sd.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        groups.setdefault(storage_key(v), []).append(k)\n",
    "\n",
    "print(\"## Shared Storage:\")\n",
    "for k, names in groups.items():\n",
    "    if len(names) > 1:\n",
    "        print(names)\n",
    "\n",
    "print(\"\\n## Unique Storage:\")\n",
    "for k, names in groups.items():\n",
    "    if len(names) == 1:\n",
    "        print(names)"
   ],
   "id": "68a48e821ad7b88c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Shared Storage:\n",
      "\n",
      "## Unique Storage:\n",
      "['lm_head_w']\n",
      "['scalars']\n",
      "['embed.weight']\n",
      "['blocks.0.attn.qkvo_w']\n",
      "['blocks.0.mlp.fc_w']\n",
      "['blocks.0.mlp.proj_w']\n",
      "['blocks.1.attn.qkvo_w']\n",
      "['blocks.1.mlp.fc_w']\n",
      "['blocks.1.mlp.proj_w']\n",
      "['blocks.2.attn.qkvo_w']\n",
      "['blocks.2.mlp.fc_w']\n",
      "['blocks.2.mlp.proj_w']\n",
      "['blocks.3.attn.qkvo_w']\n",
      "['blocks.3.mlp.fc_w']\n",
      "['blocks.3.mlp.proj_w']\n",
      "['blocks.4.attn.qkvo_w']\n",
      "['blocks.4.mlp.fc_w']\n",
      "['blocks.4.mlp.proj_w']\n",
      "['blocks.5.attn.qkvo_w']\n",
      "['blocks.5.mlp.fc_w']\n",
      "['blocks.5.mlp.proj_w']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:36:20.324045Z",
     "start_time": "2025-11-03T21:36:18.378636Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f0de43a5cf8c41f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_embeds.0.weight tensor([[ 0.4043,  3.7188, -3.3281,  ..., -1.9922, -1.2031, -1.6875],\n",
      "        [-0.1475,  1.3594, -0.7344,  ..., -2.0781, -0.4492, -2.5938],\n",
      "        [-4.5625, -1.3594, -2.7031,  ..., -1.8359,  2.7656,  1.1406],\n",
      "        ...,\n",
      "        [-0.2051, -2.9062,  1.2578,  ..., -1.2969, -1.9453,  0.8047],\n",
      "        [ 3.7656, -0.0649, -0.7500,  ...,  0.2812,  1.1406,  1.2969],\n",
      "        [-0.8828, -2.5000,  0.1245,  ..., -0.2129, -0.0688,  5.3750]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.1.weight tensor([[-0.8867,  1.6797, -3.6406,  ..., -0.1001,  2.1875, -3.1875],\n",
      "        [ 0.4961,  0.0591, -0.3613,  ..., -2.6875, -0.9570, -2.3750],\n",
      "        [ 1.5781,  4.4062,  0.6523,  ..., -1.7969,  0.9922, -0.8164],\n",
      "        ...,\n",
      "        [-0.2148,  1.7969,  0.9297,  ..., -1.1016,  1.6875,  0.8438],\n",
      "        [-0.5898,  0.4082, -1.0547,  ...,  1.0859,  0.3711,  0.5039],\n",
      "        [ 3.1406,  1.5469,  0.8164,  ..., -1.2031, -2.1250, -0.3438]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.2.weight tensor([[-2.5625e+00, -5.0781e-01, -2.0938e+00,  ...,  2.1719e+00,\n",
      "          2.2188e+00, -3.4424e-02],\n",
      "        [ 1.9844e+00,  1.2734e+00, -2.1875e+00,  ..., -1.9766e+00,\n",
      "         -1.9531e+00, -6.2500e-01],\n",
      "        [ 1.5547e+00, -2.3281e+00, -2.8750e+00,  ...,  9.6130e-04,\n",
      "         -7.2754e-02, -2.9102e-01],\n",
      "        ...,\n",
      "        [-1.2578e+00, -1.1562e+00, -6.3281e-01,  ...,  5.1953e-01,\n",
      "         -3.8867e-01, -8.9453e-01],\n",
      "        [-1.7031e+00, -2.5938e+00, -5.2344e-01,  ..., -2.0156e+00,\n",
      "         -3.0625e+00,  4.1406e-01],\n",
      "        [ 5.0391e-01,  7.2656e-01,  6.3477e-02,  ..., -1.1562e+00,\n",
      "          1.4609e+00,  4.4336e-01]], device='mps:0', dtype=torch.bfloat16)\n",
      "bytes by dtype: {'torch.float32': '98.25 MiB', 'torch.bfloat16': '232.32 MiB'}\n",
      "sum(storage nbytes): 330.57 MiB\n",
      "value_embeds.0.weight tensor([[ 0.4043,  3.7188, -3.3281,  ..., -1.9922, -1.2031, -1.6875],\n",
      "        [-0.1475,  1.3594, -0.7344,  ..., -2.0781, -0.4492, -2.5938],\n",
      "        [-4.5625, -1.3594, -2.7031,  ..., -1.8359,  2.7656,  1.1406],\n",
      "        ...,\n",
      "        [-0.2051, -2.9062,  1.2578,  ..., -1.2969, -1.9453,  0.8047],\n",
      "        [ 3.7656, -0.0649, -0.7500,  ...,  0.2812,  1.1406,  1.2969],\n",
      "        [-0.8828, -2.5000,  0.1245,  ..., -0.2129, -0.0688,  5.3750]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.1.weight tensor([[-0.8867,  1.6797, -3.6406,  ..., -0.1001,  2.1875, -3.1875],\n",
      "        [ 0.4961,  0.0591, -0.3613,  ..., -2.6875, -0.9570, -2.3750],\n",
      "        [ 1.5781,  4.4062,  0.6523,  ..., -1.7969,  0.9922, -0.8164],\n",
      "        ...,\n",
      "        [-0.2148,  1.7969,  0.9297,  ..., -1.1016,  1.6875,  0.8438],\n",
      "        [-0.5898,  0.4082, -1.0547,  ...,  1.0859,  0.3711,  0.5039],\n",
      "        [ 3.1406,  1.5469,  0.8164,  ..., -1.2031, -2.1250, -0.3438]],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n",
      "value_embeds.2.weight tensor([[-2.5625e+00, -5.0781e-01, -2.0938e+00,  ...,  2.1719e+00,\n",
      "          2.2188e+00, -3.4424e-02],\n",
      "        [ 1.9844e+00,  1.2734e+00, -2.1875e+00,  ..., -1.9766e+00,\n",
      "         -1.9531e+00, -6.2500e-01],\n",
      "        [ 1.5547e+00, -2.3281e+00, -2.8750e+00,  ...,  9.6130e-04,\n",
      "         -7.2754e-02, -2.9102e-01],\n",
      "        ...,\n",
      "        [-1.2578e+00, -1.1562e+00, -6.3281e-01,  ...,  5.1953e-01,\n",
      "         -3.8867e-01, -8.9453e-01],\n",
      "        [-1.7031e+00, -2.5938e+00, -5.2344e-01,  ..., -2.0156e+00,\n",
      "         -3.0625e+00,  4.1406e-01],\n",
      "        [ 5.0391e-01,  7.2656e-01,  6.3477e-02,  ..., -1.1562e+00,\n",
      "          1.4609e+00,  4.4336e-01]], device='mps:0', dtype=torch.bfloat16)\n",
      "bytes by dtype: {'torch.float32': '98.25 MiB', 'torch.bfloat16': '232.32 MiB'}\n",
      "sum(storage nbytes): 330.57 MiB\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
