{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prelude",
   "id": "1a1ed627a0d69b55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:31:37.298295Z",
     "start_time": "2025-10-23T19:31:37.294953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prelude():\n",
    "    def _clear_vars():\n",
    "        for _n in list(globals()):\n",
    "            if _n != \"prelude\" and _n.startswith(\"_\") and _n not in (\"In\",\"Out\",\"get_ipython\",\"exit\",\"quit\"):\n",
    "                del globals()[_n]\n",
    "\n",
    "    _clear_vars()\n",
    "\n",
    "    G = globals()\n",
    "\n",
    "    import contextlib\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def dev_sync(device: str):\n",
    "        is_cuda = torch.cuda.is_available() and str(device).startswith(\"cuda\")\n",
    "        is_mps = hasattr(torch, \"mps\") and torch.mps.is_available() and str(device).startswith(\"mps\")\n",
    "        if is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "        elif is_mps:\n",
    "            torch.mps.synchronize()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if is_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "            elif is_mps:\n",
    "                torch.mps.synchronize()\n",
    "    G[\"dev_sync\"] = dev_sync\n",
    "\n",
    "# pretend export to help static analysis\n",
    "def dev_sync(device: str):\n",
    "    pass"
   ],
   "id": "de0d4335326716d2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device Copy Overhead",
   "id": "6fad247121126c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:31:38.976319Z",
     "start_time": "2025-10-23T19:31:38.974307Z"
    }
   },
   "cell_type": "code",
   "source": "prelude()",
   "id": "93f3ca7b0a98e429",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-23T19:31:48.355193Z",
     "start_time": "2025-10-23T19:31:40.937405Z"
    }
   },
   "source": [
    "import time, torch, torch.nn.functional as F\n",
    "V = 50000\n",
    "dtype = torch.bfloat16\n",
    "iters = 2048\n",
    "x=torch.randn(V, device=\"mps\", dtype=dtype)\n",
    "s=time.perf_counter()\n",
    "for _ in range(iters): _ = int(torch.argmax(x).item())\n",
    "torch.mps.synchronize(); print(\"1-scalar copy:\", time.perf_counter()-s)\n",
    "\n",
    "p = F.softmax(x, -1).to(\"cpu\"); s=time.perf_counter()\n",
    "for _ in range(iters): _ = int(torch.multinomial(p,1))  # simulating CPU sampler\n",
    "print(\"CPU multinomial -> CPU scalar:\", time.perf_counter()-s)\n",
    "\n",
    "\n",
    "p = F.softmax(x, -1).to(\"mps\"); s=time.perf_counter()\n",
    "for _ in range(iters): _ = int(torch.multinomial(p,1))\n",
    "print(\"MPS multinomial -> CPU scalar:\", time.perf_counter()-s)\n",
    "\n",
    "p =F.softmax(x, -1).to(\"mps\"); s=time.perf_counter()\n",
    "for _ in range(iters): _ = int(torch.multinomial(p,1))\n",
    "print(\"MPS multinomial -> CPU scalar:\", time.perf_counter()-s)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-scalar copy: 0.5925095830170903\n",
      "CPU multinomial -> CPU scalar: 1.8556773330201395\n",
      "MPS multinomial -> CPU scalar: 2.4886934169917367\n",
      "MPS multinomial -> CPU scalar: 2.4192957500054035\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rotary DTypes",
   "id": "8b0502573dc9188b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:31:49.779964Z",
     "start_time": "2025-10-23T19:31:49.777879Z"
    }
   },
   "cell_type": "code",
   "source": "prelude()",
   "id": "e3e932d3e6f2686",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:31:52.531300Z",
     "start_time": "2025-10-23T19:31:52.525399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "def _apply_rope(x, cos, sin):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat((y1, y2), dim=-1)\n",
    "\n",
    "class RotaryPositionEncoding(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len, dtype):\n",
    "        super().__init__()\n",
    "        half = head_dim // 2\n",
    "        keep = head_dim // 4\n",
    "        base = 1024\n",
    "        if keep == 0:\n",
    "            angular = (1 / base) ** torch.linspace(0, 1, steps=half, dtype=dtype)\n",
    "        else:\n",
    "            active = (1 / base) ** torch.linspace(0, 1, steps=keep, dtype=dtype)\n",
    "            angular = torch.cat([active, active.new_zeros(half - keep)])\n",
    "        self.inv_freq = nn.Buffer(angular, persistent=False)\n",
    "        t = torch.arange(max_seq_len, dtype=dtype)\n",
    "        theta = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        self.cos = nn.Buffer(theta.cos().to(dtype), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin().to(dtype), persistent=False)\n",
    "        self._max_seq_len = int(max_seq_len)\n",
    "\n",
    "    def forward(self, x_BTHD):\n",
    "        L = x_BTHD.size(-3)\n",
    "        cos = self.cos[:L][None, :L, None, :]\n",
    "        sin = self.sin[:L][None, :L, None, :]\n",
    "        return _apply_rope(x_BTHD, cos, sin)\n",
    "\n"
   ],
   "id": "b27ca30b5492578",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T20:20:20.041743Z",
     "start_time": "2025-10-23T20:20:16.883748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import perf_counter as p\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"mps\"\n",
    "rotary_dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "ref_dtype = torch.float32\n",
    "B, T, H, D, iters, max_seq_len = 1, 1024, 8, 128, 2048, 65536\n",
    "ref_rotary = RotaryPositionEncoding(D, max_seq_len, dtype=ref_dtype).to(device)\n",
    "\n",
    "for dtype in rotary_dtypes:\n",
    "    rotary = RotaryPositionEncoding(D, max_seq_len, dtype=dtype).to(device)\n",
    "    X = torch.randn(B, T, H, D, dtype=dtype, device=device)\n",
    "    with dev_sync(device):\n",
    "        t0 = p()\n",
    "        for i in range(iters):\n",
    "            rotary(X)\n",
    "    dur = p() - t0\n",
    "    print(f\"Rotary [{dtype}] | Avg dur/iter: {dur/iters:.6f}s\")\n",
    "\n",
    "    S = torch.zeros(iters, dtype=ref_dtype, device=device)\n",
    "    for i in range(iters):\n",
    "        Y = torch.randn(B, T, H, D, dtype=ref_dtype, device=device)\n",
    "        approx = rotary(Y.to(dtype)).to(ref_dtype)\n",
    "        ref_res = ref_rotary(Y)\n",
    "        dist = F.cosine_similarity(ref_res, approx, dim=-1).mean()\n",
    "        S[i] = dist\n",
    "    print(f\"Rotary [{dtype}] | Mean cosine similarity: {S.mean().item():.6f}\")\n"
   ],
   "id": "5549b74b14801255",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotary [torch.float32] | Avg dur/iter: 0.000122s\n",
      "Rotary [torch.float32] | Mean cosine similarity: 1.000000\n",
      "Rotary [torch.float16] | Avg dur/iter: 0.000104s\n",
      "Rotary [torch.float16] | Mean cosine similarity: 0.999386\n",
      "Rotary [torch.bfloat16] | Avg dur/iter: 0.000103s\n",
      "Rotary [torch.bfloat16] | Mean cosine similarity: 0.959034\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
