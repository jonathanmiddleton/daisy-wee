# Pretraining checkpoint path
init_checkpoint: "checkpoints/state_step_100000.pt"
# Instruct SFT training configuration
train_files: "data/instruct_mix/instruct_train_*.bin"
val_files: "data/instruct_mix/instruct_val_*.bin"
max_seq_len: 65536      # 64 * 1024
val_seq_len: 262144       # 64 * 1024
num_iterations: 50        # assumes 8x GPUs, 400 for 1x GPU
cooldown_frac: 0.9

# Common settings
vocab_size: 50257
val_tokens: 10485760
val_loss_every: 125
save_checkpoint: true
num_layers: 16
num_heads: 8
model_dim: 1024

model_type: gpt2
