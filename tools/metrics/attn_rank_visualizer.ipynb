{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Rank Visualizer — per head or whole layer\n",
    "\n",
    "This notebook is based on the paper [When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models](https://arxiv.org/html/2404.08634v3).\n",
    "\n",
    "1. Build the attention matrix `A` for a chosen head from the `CausalSelfAttention` module.\n",
    "2. Measure an **effective rank**: the smallest `k` whose top singular values explain **90%** of the matrix energy.\n",
    "3. Measure **single‑column‑ness**: the fewest columns needed to cover **90%** of the squared entries of `A`.\n",
    "4. Plot a heatmap of `A` and a per‑head rank profile.\n",
    "\n",
    "**Assumptions**\n",
    "- Uses `models.gpt2.attention.CausalSelfAttention`'s merged QKVO weights in `qkvo_w` of shape `(4, num_heads*head_dim, dim)`.\n",
    "- Hidden states `X` of shape `(T, dim)` that enter this attention block (from the previous layer).\n",
    "\n",
    "**Tip**: Start with short sequences (e.g., `T ≤ 128`) because SVD (Singular Value Decomposition) scales cubically with `T`.\n"
   ],
   "id": "4c31ed4d591a20e9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models.gpt2.attention import CausalSelfAttention\n",
    "torch.set_printoptions(precision=4, sci_mode=True)\n",
    "device = 'cpu'\n",
    "CHECKPOINT_PATH = \"/Users/jonathanmiddleton/models/checkpoints/350m-instruct/20251011T1549-val2.770-step005000-run1-best.pt\"\n",
    "LAYER_ID = 4\n"
   ],
   "id": "6647e29f49be4ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the attention matrix `A`\n",
    "`A` is computed as a row‑wise softmax of the scaled score matrix. Rows index **query** positions; columns index **key** positions.\n"
   ],
   "id": "e7a9585db312608d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def _row_softmax_with_causal_mask(scores: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Row‑wise softmax. scores: (H, T, T).\"\"\"\n",
    "    T = scores.shape[-1]\n",
    "    mask = torch.triu(torch.ones((T, T), dtype=torch.bool, device=scores.device), diagonal=1)\n",
    "    scores = scores.masked_fill(mask, float('-inf'))\n",
    "    return F.softmax(scores, dim=-1)\n",
    "\n",
    "def attention_matrix_from_attn(attn: CausalSelfAttention, X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute A for a single CausalSelfAttention module with merged QKVO weights.\n",
    "    - attn.qkvo_w: (4, num_heads*head_dim, dim) with indices 0=Q, 1=K, 2=V, 3=O\n",
    "    - attn.num_heads, attn.head_dim\n",
    "    - X: (T, dim) hidden states entering this attention block\n",
    "    Returns: A of shape (num_heads, T, T)\n",
    "    \"\"\"\n",
    "    assert hasattr(attn, 'qkvo_w'), 'Expected attn.qkvo_w merged weights.'\n",
    "    H = int(attn.num_heads)\n",
    "    Dh = int(attn.head_dim)\n",
    "    T, D = X.shape\n",
    "\n",
    "    # Extract Q and K weights and compute projected states.\n",
    "    Wq = attn.qkvo_w[0].to(dtype=torch.float32)  # (H*Dh, D)\n",
    "    Wk = attn.qkvo_w[1].to(dtype=torch.float32)  # (H*Dh, D)\n",
    "    Xf = X.to(dtype=torch.float32)\n",
    "    Q = Xf @ Wq.T  # (T, H*Dh)\n",
    "    K = Xf @ Wk.T  # (T, H*Dh)\n",
    "\n",
    "    # Reshape to per‑head tensors: (H, T, Dh)\n",
    "    Q = Q.view(T, H, Dh).transpose(0, 1).contiguous()\n",
    "    K = K.view(T, H, Dh).transpose(0, 1).contiguous()\n",
    "\n",
    "    Q = attn.rotary(Q)\n",
    "    K = attn.rotary(K)\n",
    "\n",
    "    # Scores and A: (H, T, T)\n",
    "    scores = Q @ K.transpose(-1, -2) * attn.attn_scale\n",
    "    A = _row_softmax_with_causal_mask(scores)[0] # B,H,T,T -> H,T,T\n",
    "    return A\n"
   ],
   "id": "81c202ad0d0dd198",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Metrics: effective rank (90%) and single‑column mass (90%)\n- **effective_rank90(A)**: the smallest `k` so that the top `k` singular values of `A` explain at least 90% of the squared entries.\n- **columns90(A)**: the fewest columns needed so that their squared entries sum to at least 90% of the squared entries of `A`.\n\nBoth are computed per head. Use small `T` during exploration for speed.\n",
   "id": "868e52b25d4eceea"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def effective_rank90(A_2d: torch.Tensor) -> int:\n",
    "    \"\"\"A_2d: (T, T).\"\"\"\n",
    "    # Use float64 on CPU for numerical stability\n",
    "    S = torch.linalg.svdvals(A_2d.to(dtype=torch.float64, device=device))  # singular values\n",
    "    S2 = S**2\n",
    "    total = float(S2.sum())\n",
    "    csum = torch.cumsum(S2, dim=0)\n",
    "    k = int(torch.searchsorted(csum, 0.90 * total).item()) + 1\n",
    "    return k\n",
    "\n",
    "def columns90(A_2d: torch.Tensor) -> int:\n",
    "    \"\"\"Fewest columns whose squared entries cover 90% of total squared entries.\"\"\"\n",
    "    col_mass = (A_2d**2).sum(dim=0)  # (T,)\n",
    "    vals, _ = torch.sort(col_mass.to(dtype=torch.float64, device=device), descending=True)\n",
    "    csum = torch.cumsum(vals, dim=0)\n",
    "    total = float(vals.sum())\n",
    "    m = int(torch.searchsorted(csum, 0.90 * total).item()) + 1\n",
    "    return m\n",
    "\n",
    "def per_head_metrics(A: torch.Tensor):\n",
    "    \"\"\"A: (H, T, T). Returns lists: ranks, masses, and their maxima.\"\"\"\n",
    "    H = A.shape[0]\n",
    "    ranks, masses = [], []\n",
    "    for h in range(H):\n",
    "        r = effective_rank90(A[h])\n",
    "        m = columns90(A[h])\n",
    "        ranks.append(r)\n",
    "        masses.append(m)\n",
    "    return ranks, masses, max(ranks) if ranks else None\n"
   ],
   "id": "ee6b8ea16380c74b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Plot helpers\nWe use plain Matplotlib (no seaborn). Each chart uses its own figure.\n",
   "id": "493bb9515511768"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def show_attention_heatmap(A: torch.Tensor, head: int=0, title: str=None):\n    \"\"\"A: (H, T, T) or (T, T).\"\"\"\n    plt.figure()\n    if A.dim() == 3:\n        M = A[head].detach().cpu().numpy()\n    else:\n        M = A.detach().cpu().numpy()\n    plt.imshow(M, aspect='auto')\n    plt.colorbar()\n    plt.xlabel('Key position j')\n    plt.ylabel('Query position i')\n    if title:\n        plt.title(title)\n    plt.show()\n\ndef plot_head_ranks(ranks, title: str='Effective rank (90%) per head'):\n    plt.figure()\n    xs = list(range(len(ranks)))\n    plt.plot(xs, ranks, marker='o')\n    plt.xlabel('Head index')\n    plt.ylabel('Rank-90%')\n    plt.title(title)\n    plt.show()\n\ndef plot_head_masses(masses, title: str='Fewest columns for 90% mass per head'):\n    plt.figure()\n    xs = list(range(len(masses)))\n    plt.plot(xs, masses, marker='o')\n    plt.xlabel('Head index')\n    plt.ylabel('#columns for 90% mass')\n    plt.title(title)\n    plt.show()\n",
   "id": "8cfe96e800571e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from models import model_from_spec\n",
    "from models.gpt2.gpt_core import GPT2Core\n",
    "import tiktoken\n",
    "model: GPT2Core = model_from_spec('gpt2_350m', device=device).eval()\n",
    "attn: CausalSelfAttention = model.blocks[0].attn\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "s = \"What is the meaning of life?\"\n",
    "tokens = enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "p = torch.tensor(tokens, dtype=torch.long)[None,:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    if CHECKPOINT_PATH is not None:\n",
    "        from tools.checkpoint import load_checkpoint, apply_model_state\n",
    "        ckpt = load_checkpoint(CHECKPOINT_PATH, map_location=device)\n",
    "        state_dict = ckpt.model\n",
    "        apply_model_state(model, state_dict, strict=True)\n",
    "    model.prefill_batch(p, 256)\n",
    "    block = model.blocks[LAYER_ID]\n",
    "    X: torch.Tensor = block.in_t[0] # (T, dim)\n",
    "\n",
    "    B = 1\n",
    "    H = int(attn.num_heads)\n",
    "    Dh = int(attn.head_dim)\n",
    "    T, D = X.shape\n",
    "\n",
    "    # Extract Q and K weights and compute projected states.\n",
    "    Wq = attn.qkvo_w[0].to(dtype=torch.float32)  # (H*Dh, D)\n",
    "    Wk = attn.qkvo_w[1].to(dtype=torch.float32)  # (H*Dh, D)\n",
    "    Xf = X.to(dtype=torch.float32)\n",
    "    Q = Xf @ Wq.T  # (T, H*Dh)\n",
    "    K = Xf @ Wk.T  # (T, H*Dh)\n",
    "\n",
    "    # Reshape to per‑head tensors: (B,T,H,Dh)\n",
    "    # Q = Q.view(B, H, T, Dh).transpose(1, 2).contiguous()\n",
    "    # K = K.view(B, H, T, Dh).transpose(1, 2).contiguous()\n",
    "    Q = Q.view(B, T, H, Dh).contiguous()\n",
    "    K = K.view(B, T, H, Dh).contiguous()\n",
    "\n",
    "    Q = attn.rotary(Q)\n",
    "    K = attn.rotary(K)\n",
    "\n",
    "    # Scores and A: (H, T, T)\n",
    "    scores = Q @ K.transpose(-1, -2) * attn.attn_scale\n",
    "    A = _row_softmax_with_causal_mask(scores)\n",
    "\n",
    "    A = attention_matrix_from_attn(attn, X)\n",
    "    ranks, masses, max_rank = per_head_metrics(A)\n",
    "    print('Per‑head effective ranks:', ranks)\n",
    "    print('Per‑head columns@90%:', masses)\n",
    "    print('MaxRank(layer) =', max_rank)\n",
    "    show_attention_heatmap(A, head=0, title='Attention matrix — head 0')\n",
    "    plot_head_ranks(ranks)\n",
    "    plot_head_masses(masses)"
   ],
   "id": "854c5d7e3dbd853f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch over multiple sequences (to average per head)\n",
    "- Mirror the paper's setup: sample `N=100` sequences with `T=100`, then compute per‑head averages and finally `MaxRank(l)` as the maximum head rank per layer.\n",
    "- To inspect a **single‑column** pattern directly, sort the columns of `A[h]` by their squared mass and see if the first one dominates.\n"
   ],
   "id": "9bc11108a3bf5808"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def average_per_head_over_sequences(attn, X_list) -> dict:\n",
    "    H = int(attn.num_heads)\n",
    "    sum_r = torch.zeros(H, dtype=torch.float64, device=device)\n",
    "    sum_m = torch.zeros(H, dtype=torch.float64, device=device)\n",
    "    cnt = torch.zeros(H, dtype=torch.int64, device=device)\n",
    "\n",
    "    for X in X_list:\n",
    "        A = attention_matrix_from_attn(attn, X)\n",
    "        for h in range(H):\n",
    "            r = effective_rank90(A[h])\n",
    "            m = columns90(A[h])\n",
    "            sum_r[h] += r\n",
    "            sum_m[h] += m\n",
    "            cnt[h] += 1\n",
    "\n",
    "    avg_r = (sum_r / cnt.clamp_min(1)).tolist()\n",
    "    avg_m = (sum_m / cnt.clamp_min(1)).tolist()\n",
    "    return {\n",
    "        'avg_ranks_per_head': avg_r,\n",
    "        'avg_columns90_per_head': avg_m,\n",
    "        'MaxRank_layer': max(avg_r) if len(avg_r) else None\n",
    "    }\n"
   ],
   "id": "58f059fcf082583b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from training.data_gen import DistributedDataGenerator\n",
    "\n",
    "data_loader = DistributedDataGenerator(\n",
    "    \"../../data/fineweb/fineweb_val_000000.bin\",\n",
    "    1 * 100,\n",
    "    rank = 0,\n",
    "    world_size=1,\n",
    "    device=device,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    I = [inputs[None,:] for inputs, _ in (next(data_loader) for _ in range(100))] # 100 in paper\n",
    "    Xs = [model.blocks[LAYER_ID].in_t[0] for inputs in I for _ in model.prefill_batch(inputs, 100)]\n",
    "\n",
    "    avgs = average_per_head_over_sequences(attn, Xs)\n",
    "    print(\"avg_ranks_per_head:\", avgs[\"avg_ranks_per_head\"])\n",
    "    print(\"avg_columns90_per_head\", avgs[\"avg_columns90_per_head\"])\n",
    "    print(\"MaxRank_layer:\", avgs[\"MaxRank_layer\"])"
   ],
   "id": "2dc805ab6b8b676",
   "outputs": [],
   "execution_count": null
  }
 ]
}
