{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c31ed4d591a20e9",
   "metadata": {},
   "source": [
    "# Attention Rank Visualizer — per head or whole layer\n",
    "\n",
    "This notebook is an implementation of concepts in the paper [When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models](https://arxiv.org/html/2404.08634v3).\n",
    "\n",
    "**Methodology**\n",
    "1. Build the attention matrix `A` (H,T,T) across all heads in the `CausalSelfAttention` module from a chosen layer ([see attn_rank.py line 14](attn_rank.py#L14))\n",
    "2. For each head:\n",
    "    - calculate an **effective rank**: the smallest `k` whose top singular values explain **90%** of the matrix energy ([see attn_rank.py line 45](attn_rank.py#L45))\n",
    "    - measure **single‑column‑ness**: the fewest columns needed to cover **90%** of the squared entries of `A` ([see attn_rank.py line 55](attn_rank.py#L55))\n",
    "3. Plot the heatmaps of each `A` and a per‑head rank profile [Go to heatmaps section](#heatmaps-of-attention-matrices-per-layer)\n",
    "4. Compute averages over N sequences of length T [Go to batch section](#batch-over-multiple-samples) - the paper uses `N=100` and `T=100` to offer a more stable estimate of the rank profile\n",
    "\n",
    "\n",
    "**Technical Notes**\n",
    "- `models.gpt2.attention.CausalSelfAttention` uses merged QKVO weights in `qkvo_w` of shape `(4, num_heads*head_dim, dim)`\n",
    "- hidden states `X` of shape `(B, T, dim)` that enter an attention block (from the previous layer) are exposed in `CausalSelfAttention` as `in_t` for convenience\n",
    "\n",
    "**Note**: Start with short sequences (e.g., `T ≤ 128`) because SVD (Singular Value Decomposition) scales cubically with `T`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6837be1d58919",
   "metadata": {},
   "source": "# Helpers"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:11:58.902705Z",
     "start_time": "2025-10-16T20:11:58.218873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models.gpt2.block\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=True)"
   ],
   "id": "1d871b19a845bf68",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c141280cacb35d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:11:58.935847Z",
     "start_time": "2025-10-16T20:11:58.906027Z"
    }
   },
   "source": [
    "def show_attention_heatmap(A: torch.Tensor, head: int=0, title: str=None):\n",
    "    \"\"\"A: (H, T, T) or (T, T).\"\"\"\n",
    "    plt.figure()\n",
    "    if A.dim() == 3:\n",
    "        M = A[head].detach().cpu().numpy()\n",
    "    else:\n",
    "        M = A.detach().cpu().numpy()\n",
    "    plt.imshow(M, aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Key position j')\n",
    "    plt.ylabel('Query position i')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_head_ranks(ranks, title: str='Effective rank (90%) per head'):\n",
    "    plt.figure()\n",
    "    xs = list(range(len(ranks)))\n",
    "    plt.plot(xs, ranks, marker='o')\n",
    "    plt.xlabel('Head index')\n",
    "    plt.ylabel('Rank-90%')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_head_masses(masses, title: str='Fewest columns for 90% mass per head'):\n",
    "    plt.figure()\n",
    "    xs = list(range(len(masses)))\n",
    "    plt.plot(xs, masses, marker='o')\n",
    "    plt.xlabel('Head index')\n",
    "    plt.ylabel('#columns for 90% mass')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compute_and_visualize(model, x, layer_id: int):\n",
    "    with torch.no_grad():\n",
    "        model.prefill_batch(x, 256, debug=True)\n",
    "        attn = model.blocks[layer_id].attn\n",
    "        A = attention_matrix_from_attn(attn)\n",
    "        for h in range(A.shape[0]):\n",
    "            show_attention_heatmap(A, head=h, title=f'Attention matrix — head {h}')\n",
    "        ranks, masses, max_rank = per_head_metrics(A, device=device)\n",
    "        print('Per‑head effective ranks:', ranks)\n",
    "        print('Per‑head columns@90%:', masses)\n",
    "        print('MaxRank(layer) =', max_rank)\n",
    "        plot_head_ranks(ranks)\n",
    "        plot_head_masses(masses)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model and Loader",
   "id": "764fffaba3e724c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:15:12.952172Z",
     "start_time": "2025-10-16T20:15:10.573657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.gpt2 import GPT2Core\n",
    "from tools.metrics.attn_rank import attention_matrix_from_attn, per_head_metrics, average_per_head_over_sequences\n",
    "from training.data_gen import DistributedDataGenerator\n",
    "from tools.checkpoint import model_from_checkpoint\n",
    "\n",
    "pt = \"/Users/jonathanmiddleton/models/checkpoints/350m-instruct/20251013T1953-val1.600-step000850-run1-best.pt\"\n",
    "device = 'cpu'\n",
    "seq_len = 100 # 100 in paper\n",
    "\n",
    "data_loader = DistributedDataGenerator(\n",
    "    \"../../data/fineweb/fineweb_val_000000.bin\",\n",
    "    1 * seq_len,\n",
    "    rank = 0,\n",
    "    world_size=1,\n",
    "    device=device,\n",
    ")\n",
    "# noinspection PyTypeChecker\n",
    "model: GPT2Core = model_from_checkpoint(pt, device=device).eval()\n",
    "x = next(data_loader)[0][None,:] #heatmap sample\n",
    "\n",
    "def visualize_ws(s: str) -> str:\n",
    "    return (s.replace(\" \", \"·\")       # middle dot for space\n",
    "             .replace(\"\\t\", \"⭾\")      # tab symbol\n",
    "             .replace(\"\\n\", \"⏎\"))     # newline symbol\n",
    "\n",
    "show_text = True\n",
    "if show_text:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Print a 2-column table: token id and decoded string\n",
    "    ids = x[0].tolist()\n",
    "    toks = [enc.decode([tid]) for tid in ids]\n",
    "\n",
    "    idx_w = max(len(str(i)) for i in range(len(ids)))\n",
    "    id_w  = max(len(str(t)) for t in ids)\n",
    "\n",
    "    print(f\"idx {'id':<{id_w}}  decoded\")\n",
    "    for i, (tid, tstr) in enumerate(zip(ids, toks)):\n",
    "        safe = visualize_ws(tstr).replace(\"\\r\", \"␍\")\n",
    "        print(f\"{i:<{idx_w}}  {tid:<{id_w}}  {safe}\")"
   ],
   "id": "cd7dd3c618cf2445",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx id     decoded\n",
      "0   50256  <|endoftext|>\n",
      "1   20376  Ins\n",
      "2   3874   urance\n",
      "3   5834   ·Company\n",
      "4   16691  ·Decl\n",
      "5   3565   ares\n",
      "6   13728  ·Living\n",
      "7   1869   ·Man\n",
      "8   5542   ·Dead\n",
      "9   198    ⏎\n",
      "10  20191  George\n",
      "11  38579  ·Johannes\n",
      "12  268    en\n",
      "13  318    ·is\n",
      "14  845    ·very\n",
      "15  881    ·much\n",
      "16  6776   ·alive\n",
      "17  13     .\n",
      "18  9022   ·Which\n",
      "19  318    ·is\n",
      "20  1521   ·why\n",
      "21  340    ·it\n",
      "22  373    ·was\n",
      "23  523    ·so\n",
      "24  6452   ·surprising\n",
      "25  618    ·when\n",
      "26  262    ·the\n",
      "27  5398   ·Canadian\n",
      "28  582    ·man\n",
      "29  2722   ·received\n",
      "30  257    ·a\n",
      "31  3850   ·letter\n",
      "32  9469   ·addressed\n",
      "33  564    ·�\n",
      "34  250    �\n",
      "35  2514   To\n",
      "36  262    ·the\n",
      "37  23015  ·Estate\n",
      "38  286    ·of\n",
      "39  4502   ·George\n",
      "40  38579  ·Johannes\n",
      "41  268    en\n",
      "42  13     .\n",
      "43  447    �\n",
      "44  251    �\n",
      "45  3412   ·Even\n",
      "46  517    ·more\n",
      "47  6452   ·surprising\n",
      "48  318    ·is\n",
      "49  326    ·that\n",
      "50  340    ·it\n",
      "51  1625   ·came\n",
      "52  422    ·from\n",
      "53  465    ·his\n",
      "54  5096   ·insurance\n",
      "55  1664   ·company\n",
      "56  11     ,\n",
      "57  508    ·who\n",
      "58  815    ·should\n",
      "59  1107   ·really\n",
      "60  307    ·be\n",
      "61  319    ·on\n",
      "62  1353   ·top\n",
      "63  286    ·of\n",
      "64  884    ·such\n",
      "65  1243   ·things\n",
      "66  13     .\n",
      "67  198    ⏎\n",
      "68  3844   Now\n",
      "69  428    ·this\n",
      "70  3636   ·wouldn\n",
      "71  447    �\n",
      "72  247    �\n",
      "73  83     t\n",
      "74  423    ·have\n",
      "75  587    ·been\n",
      "76  523    ·so\n",
      "77  7818   ·terrible\n",
      "78  611    ·if\n",
      "79  28293  ·Manitoba\n",
      "80  5094   ·Public\n",
      "81  17541  ·Insurance\n",
      "82  373    ·was\n",
      "83  3501   ·giving\n",
      "84  38579  ·Johannes\n",
      "85  268    en\n",
      "86  447    �\n",
      "87  247    �\n",
      "88  82     s\n",
      "89  7964   ·estate\n",
      "90  257    ·a\n",
      "91  3735   ·fat\n",
      "92  2198   ·check\n",
      "93  329    ·for\n",
      "94  465    ·his\n",
      "95  6427   ·passing\n",
      "96  1497   ·away\n",
      "97  13     .\n",
      "98  887    ·But\n",
      "99  326    ·that\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "f6444570a3d84e76",
   "metadata": {},
   "source": "# Heatmaps of Attention matrices per layer"
  },
  {
   "cell_type": "code",
   "id": "196632ebc3e33438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:12:02.074838Z",
     "start_time": "2025-10-16T20:12:02.046559Z"
    }
   },
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "_layer_dropdown = widgets.Dropdown(\n",
    "    options=list(range(len(model.blocks))),\n",
    "    value=0,\n",
    "    description='Layer:',\n",
    ")\n",
    "_run_button = widgets.Button(description='Run', button_style='primary')\n",
    "_out = widgets.Output()\n",
    "\n",
    "display(widgets.HBox([_layer_dropdown, _run_button]), _out)\n",
    "\n",
    "def _on_run_clicked(_):\n",
    "    _run_button.disabled = True\n",
    "    _run_button.description = \"Running...\"\n",
    "    _run_button.button_style = 'info'\n",
    "    try:\n",
    "        with _out:\n",
    "            clear_output(wait=True)\n",
    "            compute_and_visualize(model, x, int(_layer_dropdown.value))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        _run_button.disabled = False\n",
    "        _run_button.description = \"Run\"\n",
    "        _run_button.button_style = 'primary'\n",
    "\n",
    "_run_button.on_click(_on_run_clicked)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(Dropdown(description='Layer:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3178c2ae17d94f93bf75cdd09141df59"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89813aace4b94e1a998e2fd86e2ee4f5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "37818c450beb2844",
   "metadata": {},
   "source": [
    "# Batch over multiple samples\n",
    "Averaged over `N` sequences of length `T`.\n",
    "- Mirroring the paper's setup: sample `N=100` sequences with `T=100`, then compute per‑head averages and finally `MaxRank(l)` as the maximum head rank per layer.\n",
    "- To inspect a **single‑column** pattern directly, sort the columns of `A[h]` by their squared mass and see if the first one dominates.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8a638b6cc43253f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:12:02.088605Z",
     "start_time": "2025-10-16T20:12:02.086386Z"
    }
   },
   "source": [
    "samples = 100 # 100 in paper\n",
    "def list_f(l: list) -> str:\n",
    "    return \"[\" + \", \".join(f\"{v:.2f}\" for v in l) + \"]\"\n",
    "\n",
    "def compute_batch(layer_id: int):\n",
    "    with torch.no_grad():\n",
    "        I = [inputs[None,:] for inputs, _ in (next(data_loader) for _ in range(samples))]\n",
    "        avgs = average_per_head_over_sequences(model, I, layer_id, device=device)\n",
    "        print(\"avg_ranks_per_head: \", list_f(avgs['avg_ranks_per_head']))\n",
    "        print(\"avg_columns90_per_head: \", list_f(avgs['avg_columns90_per_head']))\n",
    "        print(f\"MaxRank_layer: {avgs['MaxRank_layer']:.2f}\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "83be0b07968f7de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:12:02.100067Z",
     "start_time": "2025-10-16T20:12:02.095174Z"
    }
   },
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "_layer_dropdown_batch = widgets.Dropdown(\n",
    "    options=list(range(len(model.blocks))),\n",
    "    value=0,\n",
    "    description='Layer:',\n",
    ")\n",
    "_run_button_batch = widgets.Button(description='Run', button_style='primary')\n",
    "_out_batch = widgets.Output()\n",
    "\n",
    "def _on_run_clicked_batch(_):\n",
    "    _run_button_batch.button_style = 'info'\n",
    "    _run_button_batch.description = \"Running...\"\n",
    "    _run_button_batch.disabled = True\n",
    "    try:\n",
    "        with _out_batch:\n",
    "            clear_output(wait=True)\n",
    "            compute_batch(int(_layer_dropdown_batch.value))\n",
    "    finally:\n",
    "        _run_button_batch.button_style = 'primary'\n",
    "        _run_button_batch.description = \"Run\"\n",
    "        _run_button_batch.disabled = False\n",
    "\n",
    "try:\n",
    "    _run_button_batch.on_click(_on_run_clicked_batch, remove=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "_run_button_batch.on_click(_on_run_clicked_batch)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([_layer_dropdown_batch, _run_button_batch]),\n",
    "    _out_batch\n",
    "]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Layer:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7c85bfd5e2c4d3cbcc2adffad8d0cfc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
